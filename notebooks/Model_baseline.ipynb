{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We aim to implement a baseline CNN-based using PyTorch. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import copy\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.checkpoint as cp\n",
    "from torchvision import datasets, models, transforms\n",
    "from sklearn.metrics import f1_score\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Download and Split: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"//mnt/disks/dades/\"\n",
    "train_data = datasets.ImageFolder('//mnt/disks/dades/train/')\n",
    "num_classes = len(train_data.classes)\n",
    "model_name = \"densenet\"  # resnet, vgg or densenet\n",
    "input_size = 224  # DenseNet Characteristic\n",
    "batch_size = 16\n",
    "feature_extract = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.Resize(input_size),\n",
    "        transforms.CenterCrop(input_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'valid': transforms.Compose([\n",
    "        transforms.Resize(input_size),\n",
    "        transforms.CenterCrop(input_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "# Create training and validation datasets\n",
    "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x), data_transforms[x]) for x in ['train', 'valid']}\n",
    "# Create training and validation dataloaders\n",
    "dataloaders_dict = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=batch_size, shuffle=True, num_workers=4) for x in ['train', 'valid']}\n",
    "class_names = image_datasets['train'].classes\n",
    "\n",
    "# Detect if we have a GPU available\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(inp):\n",
    "    inp = inp.numpy().transpose((1, 2, 0))\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    inp = std * inp + mean\n",
    "    inp = np.clip(inp, 0, 1)\n",
    "    plt.imshow(inp)\n",
    "    plt.pause(0.001)\n",
    "\n",
    "# Get a mini-batch of training data\n",
    "mini_batch = 4\n",
    "dataloaders_dict_visualize = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=mini_batch, shuffle=True, num_workers=4) for x in ['train']}\n",
    "inputs, classes = next(iter(dataloaders_dict_visualize['train']))\n",
    "\n",
    "# Make a grid from batch\n",
    "out = torchvision.utils.make_grid(inputs)\n",
    "imshow(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloaders, criterion, optimizer, num_epochs=25):\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_fscore = 0.0\n",
    "    \n",
    "    loss_train_evo=[]\n",
    "    acc_train_evo=[]\n",
    "    fs_train_evo=[]\n",
    "    \n",
    "    loss_val_evo=[]\n",
    "    acc_val_evo=[]\n",
    "    fs_val_evo=[]\n",
    "    \n",
    "    total_train=round(47626/batch_size)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        i = 0\n",
    "        print('Epoch {}/{}'.format(epoch+1, num_epochs))\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'valid']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "            \n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "            fscore = []\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    \"\"\"\n",
    "                    if i==round(0.25*total_train):\n",
    "                        print('Forward Passed 25%')\n",
    "                    if i==round(0.5*total_train):\n",
    "                        print('Forward Passed 50%')\n",
    "                    if i==round(0.75*total_train):\n",
    "                        print('Forward Passed 75%')\n",
    "                    i = i + 1\n",
    "                    \"\"\"\n",
    "                    # Get model outputs and calculate loss\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                labels_cpu = labels.cpu().numpy()\n",
    "                predictions_cpu = preds.cpu().numpy()\n",
    "                Fscore = f1_score(labels_cpu, predictions_cpu, average='macro')\n",
    "                fscore.append(Fscore)\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
    "            epoch_fscore = np.average(np.array(fscore))\n",
    "            \n",
    "            print('{} Loss: {:.4f} Acc: {:.4f} F: {:.3f}'.format(phase, epoch_loss, epoch_acc, epoch_fscore))\n",
    "            \n",
    "            if phase == 'train':\n",
    "                loss_train_evo.append(epoch_loss)\n",
    "                epoch_acc = epoch_acc.cpu().numpy()\n",
    "                acc_train_evo.append(epoch_acc)\n",
    "                fs_train_evo.append(epoch_fscore)                \n",
    "            else:\n",
    "                loss_val_evo.append(epoch_loss)\n",
    "                epoch_acc = epoch_acc.cpu().numpy()\n",
    "                acc_val_evo.append(epoch_acc)\n",
    "                fs_val_evo.append(epoch_fscore) \n",
    "                \n",
    "            # deep copy the model\n",
    "            if phase == 'valid' and epoch_fscore > best_fscore:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, loss_train_evo, acc_train_evo, fs_train_evo, loss_val_evo, acc_val_evo, fs_val_evo\n",
    "\n",
    "# sets the .requires_grad attribute of the parameters in the model to False when we are feature extracting\n",
    "def set_parameter_requires_grad(model, feature_extracting):\n",
    "    if feature_extracting:\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False\n",
    "            \n",
    "def initialize_model(model_name, num_classes, feature_extract, use_pretrained=True):\n",
    "    model_ft = None\n",
    "    input_size = 0\n",
    "\n",
    "    if model_name == \"resnet\":  # ResNet-50\n",
    "        model_ft = models.resnet50(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.fc.in_features\n",
    "        model_ft.fc = nn.Linear(num_ftrs, num_classes)\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"vgg\":  # VGG-11\n",
    "        model_ft = models.vgg11_bn(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.classifier[6].in_features\n",
    "        model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes)\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"densenet\":  # DenseNet-121\n",
    "        model_ft = models.densenet121(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.classifier.in_features\n",
    "        model_ft.classifier = nn.Linear(num_ftrs, num_classes)\n",
    "        input_size = 224\n",
    "\n",
    "    return model_ft, input_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model for this run\n",
    "model_ft, input_size = initialize_model(model_name, num_classes, feature_extract, use_pretrained=True)\n",
    "\n",
    "# Send the model to GPU\n",
    "model_ft = model_ft.to(device)\n",
    "\n",
    "params_to_update = model_ft.parameters()\n",
    "if feature_extract:\n",
    "    params_to_update = []\n",
    "    for name,param in model_ft.named_parameters():\n",
    "        if param.requires_grad == True:\n",
    "            params_to_update.append(param)\n",
    "            # print(\"\\t\",name)\n",
    "else:\n",
    "    for name,param in model_ft.named_parameters():\n",
    "        if param.requires_grad == True:\n",
    "            a=1 # print(\"\\t\",name)\n",
    "            \n",
    "# Optimizer\n",
    "optimizer_ft = optim.Adam(params_to_update, lr=3e-4)\n",
    "\n",
    "# Loss Funciton\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 5\n",
    "model_ft, loss_train, acc_train, fs_train, loss_val, acc_val, fs_val = train_model(model_ft, dataloaders_dict, criterion, optimizer_ft, num_epochs=num_epochs)\n",
    "# Save model\n",
    "# torch.save(model_ft.state_dict(),'/mnt/disks/dades/model_baseline.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metric(metric_train, metric_val, title):\n",
    "    fig, (ax) = plt.subplots(1, 1)\n",
    "    fig.suptitle(title)\n",
    "    ax.set(xlabel='epoch')\n",
    "    ax.plot(metric_train, label='Training')\n",
    "    ax.plot(metric_val, label='Validation')\n",
    "    ax.legend(loc='upper left')\n",
    "\n",
    "\"\"\"\n",
    "# Results for a GCLOUD Terminal training for 20 epochs\n",
    "\n",
    "loss_train = [1.97, 1.47, 1.26, 1.08,  0.95,  0.82,  0.72,  0.62,  0.54,  0.48,  0.43,  0.38,  0.34,  0.32,  0.30,  0.26,  0.25,  0.23,  0.22,  0.21]\n",
    "loss_val =  [1.58, 1.40, 1.30, 1.27, 1.28, 1.21, 1.34, 1.31, 1.38, 1.47, 1.46, 1.58, 1.57, 1.60, 1.71, 1.72, 1.71, 1.76, 1.78, 1.81]\n",
    "\n",
    "acc_train = [0.45, 0.57, 0.63, 0.68, 0.71, 0.74, 0.77, 0.80, 0.82, 0.85, 0.86, 0.88, 0.89, 0.90, 0.90, 0.91, 0.92, 0.92, 0.93, 0.94]\n",
    "acc_val = [0.54, 0.59, 0.62, 0.63, 0.64, 0.65, 0.64, 0.64, 0.64, 0.63, 0.63, 0.64, 0.64, 0.64, 0.63, 0.63, 0.63, 0.62, 0.63, 0.62]\n",
    "\n",
    "fs_train = [0.30,  0.42,  0.48,  0.53,  0.57,  0.61,  0.65,  0.69,  0.72,  0.75,  0.77,  0.79,  0.82,  0.83,  0.83,  0.85,  0.86,  0.87,  0.88,  0.89]\n",
    "fs_val = [0.38,  0.44,  0.46,  0.48,  0.49,  0.51,  0.49,  0.49,  0.48,  0.48,  0.49,  0.49,  0.48,  0.49,  0.48,  0.48,  0.50,  0.48,  0.48, 0.47]\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "plot_metric(loss_train, loss_val, 'Loss')\n",
    "plot_metric(acc_train, acc_val, 'Accuracy')\n",
    "plot_metric(fs_train, fs_val, 'F-Score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
